export const metadata = {
  title: 'Attention please',
  description: 'An overview of different attention variants',
  openGraph: {
    title: 'Attention please',
    description: 'An overview of different attention variants',
    images: [{ url: '/og/attention-please' }]
  }
}

<FloatingTOC
  items={[
    { text: "self-attention", href: "#self-attention" },
    { text: "causal self-attention", href: "#causal-self-attention" },
    { text: "multi-head attention", href: "#multi-head-attention" },
    { text: "multi-query attention", href: "#multi-query-attention" },
    { text: "grouped-query attention", href: "#grouped-query-attention" },
    { text: "resources", href: "#resources" }
  ]}
/>

I think it's interesting that **attention** is a quantifiable concept. Like, I don't have a way of measuring how I personally pay attention, but I imagine it's very caveman-like.

If you told me to take out the trash on Monday night because it's really full and pickup day is Tuesday, my brain would probably compile it to something like "trash, Monday night". Just the essential keywords.

But for transformer models, attention is calculable.

> An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.
> - [Attention Is All You Need](https://huggingface.co/papers/1706.03762)

This post tries to very simply and very plainly explain how the original self-attention (and several variants of it) works while eschewing as much jargon and fancy language as possible.

<Tip text="If you're interested in calculating attention by hand, click on 'Show calculations'. This really helped solidify my understanding of how attention works!" />

## self-attention [#self-attention]

Self-attention (scaled dot-product attention) computes how much each word in a sequence should "pay attention" to every other word in that same sequence. It's what makes transformers contextually aware.

You need 3 matrices to compute self-attention. These matrices are created by multiplying the <HoverWord 
  word="word embeddings"
  description="Vectors of numbers that represent words in a way that captures meaning and relationships."
/> (x) by 3 <HoverWord
  word="weight matrices"
  description="Learnable parameters that are optimized during training."
/>
(W<sub>k</sub>, W<sub>q</sub>, W<sub>v</sub>).

- Query (Q) is compared to all the Ks (including itself) to calculate how much attention to pay to each word. Q is the information you're looking for.
- Key (K) is multiplied by Q (<HoverWord
  word="dot product"
  description="A mathematical operation that compares how similar two vectors are. In the context of transformers, it compares how similar two tokens are."
/>) to produce the attention scores for each word. K is the information a word contains.
- Value (V) weights each word with their attention scores to determine what information every other word offers. V is the information you get from each word.

Before multiplying V by the attention scores, you need to *scale* the attention scores by dividing by the square root of the dimension. For example, if the dimension is 64, divide the attention scores by 8.

Scaling prevents the attention scores from becoming too large or too small. This helps the <HoverWord
  word="softmax"
  description="A function that exponentiates a value and divides by the sum of all the exponentiated values."
/> from being overwhelmed by any one word. The softmax converts the attention scores into probabilities that add up to 1.

Try calculating the self-attention score for the word `Fear` by hand for the following sequence to really get a feel for how it works.

<Collapsible trigger="Show calculations">

<Table 
  headers={["Word", "Q", "K", "V"]}
  data={[
    ["Fear", "[0.2, 0.1, 0.8]", "[0.3, 0.5, 0.2]", "[0.1, 0.7, 0.4]"],
    ["is", "[0.5, 0.2, 0.3]", "[0.1, 0.4, 0.6]", "[0.8, 0.1, 0.2]"],
  ]}
  className="my-8"
/>

1. Multiply Q<sub>Fear</sub> by K for every word in the sequence to get the attention score.

    `[0.2, 0.1, 0.8]*[0.3, 0.5, 0.2]` = `0.2*0.3 + 0.1*0.5 + 0.8*0.2` = `0.31`

    `[0.2, 0.1, 0.8]*[0.1, 0.4, 0.6]` = `0.2*0.1 + 0.1*0.4 + 0.8*0.6` = `0.54`

2. Scale the attention scores by dividing by the square root of 3 (the vector dimension).

    `[0.31/1.732, 0.54/1.732]` = `[0.179, 0.311]`

3. Apply the softmax function to convert the attention scores into probabilities that add up to 1.

    `[e^0.179 / (e^0.179 + e^0.311), e^0.311 / (e^0.179 + e^0.311)]` = `[0.467, 0.532]`

4. Weight V by the attention scores.

    `0.467*[0.1, 0.7, 0.4]` = `[0.0467, 0.3269, 0.1868]`

    `0.532*[0.8, 0.1, 0.2]` = `[0.4256, 0.0532, 0.1064]`

5. Add the weighted values together.

    `[0.0467+0.4256, 0.3269+0.0532, 0.1868+0.1064]` = `[0.4723, 0.3801, 0.2932]`

This is the final self-attention output for `Fear`.
</Collapsible>

## causal self-attention [#causal-self-attention]

Causal self-attention is used in decoder models like GPT and other modern large language models. These models predict the next word in a sequence so it is important to prevent the model from seeing the next word in the sequence.

In contrast, bidirectional attention models like BERT has full visibility of the entire sequence.

A mask is used to block attention to future words. The mask is implemented as `-inf` for words that should be blocked. This way, when softmax is applied, the `-inf` scores becomes ~0.

Try calculating the causal self-attention score for the word `is` by hand for the following sequence.

<Collapsible trigger="Show calculations">

<Table 
  headers={["Word", "Q", "K", "V"]}
  data={[
    ["Fear", "[0.2, 0.1, 0.8]", "[0.3, 0.5, 0.2]", "[0.1, 0.7, 0.4]"],
    ["is", "[0.5, 0.2, 0.3]", "[0.1, 0.4, 0.6]", "[0.8, 0.1, 0.2]"],
    ["the", "[0.1, 0.6, 0.3]", "[0.4, 0.2, 0.5]", "[0.2, 0.9, 0.1]"],
  ]}
  className="my-8"
/>

1. Multiply Q<sub>is</sub> by K for every word in the sequence to get the attention score.

    `[0.5, 0.2, 0.3]*[0.3, 0.5, 0.2]` = `0.5*0.3+0.2*0.5+0.3*0.2` = `0.31`

    `[0.5, 0.2, 0.3]*[0.1, 0.4, 0.6]` = `0.5*0.1+0.2*0.4+0.3*0.6` = `0.38`

    `[0.5, 0.2, 0.3]*[0.4, 0.2, 0.5]` = `0.5*0.4+0.2*0.2+0.3*0.5` = `0.41`

    The attention score is `[0.31, 0.38, 0.41]`.

2. Scale the attention scores by dividing by the square root of 3 (the vector dimension).

    `[0.31/1.732, 0.38/1.732, 0.41/1.732]` = `[0.179, 0.219, 0.237]`

3. Set `0.237` to `-inf` to block attention to the future word (`the`).

    `[0.179, 0.3219, -inf]`
    
    The rest of the calculation is the same as self-attention.
</Collapsible>

## multi-head attention [#multi-head-attention]

<div style={{width: '100%', height: '300px', position: 'relative', borderRadius: 16}}>
    <div style={{width: 91.47, height: 47.58, left: 407.79, top: 23.50, position: 'absolute', background: '#DBEAFE', borderRadius: 16, border: '3px #93C5FD solid'}} />
    <div style={{width: 91.47, height: 47.58, left: 580.53, top: 73.91, position: 'absolute', background: '#DBEAFE', borderRadius: 16, border: '3px #93C5FD solid'}} />
    <div style={{width: 91.47, height: 47.58, left: 580.53, top: 128, position: 'absolute', background: '#DBEAFE', borderRadius: 16, border: '3px #93C5FD solid'}} />
    <div style={{width: 91.47, height: 47.58, left: 580.53, top: 182.09, position: 'absolute', background: '#DBEAFE', borderRadius: 16, border: '3px #93C5FD solid'}} />
    <div style={{width: 79, height: 14, left: 124, top: 29, position: 'absolute', color: '#64748B', fontSize: 8, fontFamily: 'Geist Mono', fontWeight: '700', wordWrap: 'break-word'}}>scale + softmax</div>
    <div style={{width: 79, height: 14, left: 124, top: 133, position: 'absolute', color: '#64748B', fontSize: 8, fontFamily: 'Geist Mono', fontWeight: '700', wordWrap: 'break-word'}}>scale + softmax</div>
    <div style={{width: 79, height: 14, left: 124, top: 242, position: 'absolute', color: '#64748B', fontSize: 8, fontFamily: 'Geist Mono', fontWeight: '700', wordWrap: 'break-word'}}>scale + softmax</div>
    <div style={{width: 91.47, height: 47.58, left: 233.91, top: 0, position: 'absolute', background: '#FEE2E2', borderRadius: 16, border: '3px #FDA4AF solid'}} />
    <div style={{width: 91.47, height: 47.58, left: 206.73, top: 35.68, position: 'absolute', background: '#EDE9FE', borderRadius: 16, border: '3px #A78BFA solid'}} />
    <div style={{width: 54.37, height: 23.50, left: 225.13, top: 47.58, position: 'absolute', color: '#A78BFA', fontSize: 16, fontFamily: 'Geist Mono', fontWeight: '700', wordWrap: 'break-word'}}>Value</div>
    <div style={{width: 11.04, height: 23.50, left: 272.14, top: 11.89, position: 'absolute', color: '#FDA4AF', fontSize: 16, fontFamily: 'Geist Mono', fontWeight: '700', wordWrap: 'break-word'}}>W</div>
    <div style={{width: 71.65, height: 0, left: 124.32, top: 47.58, position: 'absolute', background: '#D9D9D9', outline: '1px #FBBF24 dashed', outlineOffset: '-0.50px'}}></div>
    <div style={{width: 71.65, height: 0, left: 333.31, top: 47.58, position: 'absolute', background: '#D9D9D9', outline: '1px #FBBF24 dashed', outlineOffset: '-0.50px'}}></div>
    <div style={{width: 125.12, height: 0, left: 501.82, top: 47.47, position: 'absolute', transform: 'rotate(56deg)', transformOrigin: 'top left', background: '#D9D9D9', outline: '1px #FBBF24 dashed', outlineOffset: '-0.50px'}}></div>
    <div style={{width: 72.74, height: 0, left: 499.26, top: 151.86, position: 'absolute', background: '#D9D9D9', outline: '1px #FBBF24 dashed', outlineOffset: '-0.50px'}}></div>
    <div style={{width: 129.82, height: 0, left: 500, top: 260.03, position: 'absolute', transform: 'rotate(-56deg)', transformOrigin: 'top left', background: '#D9D9D9', outline: '1px #FBBF24 dashed', outlineOffset: '-0.50px'}}></div>
    <div style={{width: 91.47, height: 47.58, left: 23.22, top: 0, position: 'absolute', background: '#FFEDD5', borderRadius: 16, border: '3px #FB923C solid'}} />
    <div style={{width: 91.47, height: 47.58, left: 0, top: 35.68, position: 'absolute', background: '#BBF7D0', borderRadius: 16, border: '3px #4ADE80 solid'}} />
    <div style={{width: 54.37, height: 23.50, left: 18.41, top: 47.58, position: 'absolute', color: '#4ADE80', fontSize: 16, fontFamily: 'Geist Mono', fontWeight: '700', wordWrap: 'break-word'}}>Query</div>
    <div style={{width: 32.85, height: 23.50, left: 52.39, top: 11.89, position: 'absolute', color: '#FB923C', fontSize: 16, fontFamily: 'Geist Mono', fontWeight: '700', wordWrap: 'break-word'}}>Key</div>
    <div style={{width: 91.47, height: 47.58, left: 407.79, top: 237.03, position: 'absolute', background: '#DBEAFE', borderRadius: 16, border: '3px #93C5FD solid'}} />
    <div style={{width: 91.47, height: 47.58, left: 233.91, top: 213.52, position: 'absolute', background: '#FEE2E2', borderRadius: 16, border: '3px #FDA4AF solid'}} />
    <div style={{width: 91.47, height: 47.58, left: 206.73, top: 249.20, position: 'absolute', background: '#EDE9FE', borderRadius: 16, border: '3px #A78BFA solid'}} />
    <div style={{width: 54.37, height: 23.50, left: 225.13, top: 261.10, position: 'absolute', color: '#A78BFA', fontSize: 16, fontFamily: 'Geist Mono', fontWeight: '700', wordWrap: 'break-word'}}>Value</div>
    <div style={{width: 11.04, height: 23.50, left: 272.14, top: 225.42, position: 'absolute', color: '#FDA4AF', fontSize: 16, fontFamily: 'Geist Mono', fontWeight: '700', wordWrap: 'break-word'}}>W</div>
    <div style={{width: 71.65, height: 0, left: 124.32, top: 261.10, position: 'absolute', background: '#D9D9D9', outline: '1px #FBBF24 dashed', outlineOffset: '-0.50px'}}></div>
    <div style={{width: 71.65, height: 0, left: 333.31, top: 261.10, position: 'absolute', background: '#D9D9D9', outline: '1px #FBBF24 dashed', outlineOffset: '-0.50px'}}></div>
    <div style={{width: 91.47, height: 47.58, left: 23.22, top: 213.52, position: 'absolute', background: '#FFEDD5', borderRadius: 16, border: '3px #FB923C solid'}} />
    <div style={{width: 91.47, height: 47.58, left: 0, top: 249.20, position: 'absolute', background: '#BBF7D0', borderRadius: 16, border: '3px #4ADE80 solid'}} />
    <div style={{width: 54.37, height: 23.50, left: 18.41, top: 261.10, position: 'absolute', color: '#4ADE80', fontSize: 16, fontFamily: 'Geist Mono', fontWeight: '700', wordWrap: 'break-word'}}>Query</div>
    <div style={{width: 32.85, height: 23.50, left: 52.39, top: 225.42, position: 'absolute', color: '#FB923C', fontSize: 16, fontFamily: 'Geist Mono', fontWeight: '700', wordWrap: 'break-word'}}>Key</div>
    <div style={{width: 91.47, height: 47.58, left: 407.79, top: 128, position: 'absolute', background: '#DBEAFE', borderRadius: 16, border: '3px #93C5FD solid'}} />
    <div style={{width: 91.47, height: 47.58, left: 233.91, top: 104.50, position: 'absolute', background: '#FEE2E2', borderRadius: 16, border: '3px #FDA4AF solid'}} />
    <div style={{width: 91.47, height: 47.58, left: 206.73, top: 140.18, position: 'absolute', background: '#EDE9FE', borderRadius: 16, border: '3px #A78BFA solid'}} />
    <div style={{width: 54.37, height: 23.50, left: 225.13, top: 152.07, position: 'absolute', color: '#A78BFA', fontSize: 16, fontFamily: 'Geist Mono', fontWeight: '700', wordWrap: 'break-word'}}>Value</div>
    <div style={{width: 11.04, height: 23.50, left: 272.14, top: 116.39, position: 'absolute', color: '#FDA4AF', fontSize: 16, fontFamily: 'Geist Mono', fontWeight: '700', wordWrap: 'break-word'}}>W</div>
    <div style={{width: 71.65, height: 0, left: 124.32, top: 152.07, position: 'absolute', background: '#D9D9D9', outline: '1px #FBBF24 dashed', outlineOffset: '-0.50px'}}></div>
    <div style={{width: 71.65, height: 0, left: 333.31, top: 152.07, position: 'absolute', background: '#D9D9D9', outline: '1px #FBBF24 dashed', outlineOffset: '-0.50px'}}></div>
    <div style={{width: 91.47, height: 47.58, left: 23.22, top: 104.50, position: 'absolute', background: '#FFEDD5', borderRadius: 16, border: '3px #FB923C solid'}} />
    <div style={{width: 91.47, height: 47.58, left: 0, top: 140.18, position: 'absolute', background: '#BBF7D0', borderRadius: 16, border: '3px #4ADE80 solid'}} />
    <div style={{width: 54.37, height: 23.50, left: 18.41, top: 152.07, position: 'absolute', color: '#4ADE80', fontSize: 16, fontFamily: 'Geist Mono', fontWeight: '700', wordWrap: 'break-word'}}>Query</div>
    <div style={{width: 32.85, height: 23.50, left: 52.39, top: 116.39, position: 'absolute', color: '#FB923C', fontSize: 16, fontFamily: 'Geist Mono', fontWeight: '700', wordWrap: 'break-word'}}>Key</div>
</div>

The original self-attention is a single head of attention. Multi-head attention (MHA) adds more heads to increase model capacity and learn other patterns (grammar, repeated words, etc.).

The calculations are the same as self-attention, but the <HoverWord
  word="embedding size"
  description="The size of the vector used to represent a word. For example, Gemma 2B has an embedding size of 2048."
/> is split by the number of heads. Each head independently computes scaled dot-product attention, the outputs are concatenated, and multipled by a weight matrix to combine all the information.

[DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1) use MHA.

Try calculating the multi-head attention score for the word `Fear` by hand for the following sequence.

<Collapsible trigger="Show calculations">

<Table 
  title="Head 1"
  headers={["Word", "Q", "K", "V"]}
  data={[
    ["Fear", "2.0", "2.0", "2.0"],
    ["is", "3.0", "3.0", "3.0"],
  ]}
  className="my-8"
/>

<Table 
  title="Head 2"
  headers={["Word", "Q", "K", "V"]}
  data={[
    ["Fear", "4.0", "1.0", "3.0"],
    ["is", "6.0", "1.5", "4.5"],
  ]}
  className="my-8"
/>

1. For both heads, multiply Q<sub>Fear</sub> by K for every word in the sequence to get the attention score.

    <div className="flex gap-8 items-start">
    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-green-500">Head 1</span></div>

    `[2.0*2.0, 2.0*3.0]` = `[4.0, 6.0]`

    </div>

    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-orange-500">Head 2</span></div>

    `[4.0*1.0, 4.0*1.5]` = `[4.0, 6.0]`

    </div>
    </div>

2. Scale the attention scores of each head by dividing by the square root of 2 (the vector dimension).

    <div className="flex gap-8 items-start">
    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-green-500">Head 1</span></div>

    `[4.0/1.414, 6.0/1.414]` = `[2.828, 4.243]`

    </div>

    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-orange-500">Head 2</span></div>

    `[4.0/1.414, 6.0/1.414]` = `[2.828, 4.243]`

    </div>
    </div>

3. Apply the softmax function to convert the attention scores into probabilities that add up to 1.

    <div className="flex gap-8 items-start">
    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-green-500">Head 1</span></div>

    `[e^2.828 / (e^2.828 + e^4.243), e^4.243 / (e^2.828 + e^4.243)]` = `[0.195, 0.804]`

    </div>

    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-orange-500">Head 2</span></div>

    `[e^2.828 / (e^2.828 + e^4.243), e^4.243 / (e^2.828 + e^4.243)]` = `[0.195, 0.804]`

    </div>
    </div>

3. Weight V by the attention scores.

    <div className="flex gap-8 items-start">
    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-green-500">Head 1</span></div>

    `0.195*2.0`+`0.804*3.0` = `2.802`

    </div>

    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-orange-500">Head 2</span></div>

    `0.195*3.0`+`0.804*4.5` = `4.203`

    </div>
    </div>

4. Concatenate the outputs of each head and multiply by a <HoverWord
  word="transposed"
  description="The transpose of a matrix switches the rows and columns. A m x n matrix becomes a n x m matrix."
/> weight matrix (Wᵀ) to combine the information.

    `[2.802, 4.203]*[1.0, 1.0]ᵀ` = `4.203`

This is the final multi-head attention output for `Fear`.

</Collapsible>

## multi-query attention [#multi-query-attention]

[Multi-query attention](https://huggingface.co/papers/1911.02150) (MQA) is the same as MHA except all Qs share the same K and V.

MQA has the advantage of being more memory efficient and faster at decoding. Each head doesn't need to store a separate K and V. This makes an especially big difference for really long sequences.

[Gemma 2B](https://huggingface.co/google/gemma-2b) use MQA.

Try calculating the multi-query attention score for the word `Fear` by hand for the following sequence.

<Collapsible trigger="Show calculations">

<Table 
  title="Head 1"
  headers={["Word", "Q", "K", "V"]}
  data={[
    ["Fear", "2.0", "1.0", "3.0"],
    ["is", "3.0", "1.5", "4.5"],
  ]}
  className="my-8"
/>

<Table 
  title="Head 2"
  headers={["Word", "Q", "K", "V"]}
  data={[
    ["Fear", "4.0", "1.0", "3.0"],
    ["is", "6.0", "1.5", "4.5"],
  ]}
  className="my-8"
/>

1. For both heads, multiply Q<sub>Fear</sub> by K for every word in the sequence to get the attention score.
  
    <div className="flex gap-8 items-start">
    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-green-500">Head 1</span></div>

    `[2.0*1.0, 2.0*1.5]` = `[2.0, 3.0]`

    </div>

    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-orange-500">Head 2</span></div>

    `[4.0*1.0, 6.0*1.5]` = `[4.0, 9.0]`
    </div>
    </div>

2. Scale the attention scores of each head by dividing by the square root of 2 (the vector dimension).

    <div className="flex gap-8 items-start">
    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-green-500">Head 1</span></div>

    `[2.0/1.414, 3.0/1.414]` = `[1.414, 2.121]`

    </div>

    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-orange-500">Head 2</span></div>

    `[4.0/1.414, 9.0/1.414]` = `[2.828, 6.364]`

    </div>
    </div>

3. Apply the softmax function to convert the attention scores into probabilities that add up to 1.

    <div className="flex gap-8 items-start">
    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-green-500">Head 1</span></div>

    `[e^1.414 / (e^1.414 + e^2.121), e^2.121 / (e^1.414 + e^2.121)]` = `[0.33, 0.67]`

    </div>

    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-orange-500">Head 2</span></div>

    `[e^2.828 / (e^2.828 + e^6.364), e^6.364 / (e^2.828 + e^6.364)]` = `[0.195, 0.971]`

    </div>
    </div>

4. Weight V by the attention scores.

    <div className="flex gap-8 items-start">
    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-green-500">Head 1</span></div>

    `0.33*3.0`+`0.67*4.5` = `4.01`

    </div>

    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-orange-500">Head 2</span></div>

    `0.195*3.0`+`0.971*4.5` = `4.95`

    </div>
    </div>

5. Concatenate the outputs of each head and multiply by a weight matrix to combine the information.

    `[4.01, 4.95]*[[0.5, 1.0], [1.5, 1.0]]` = `[9.43, 8.96]`

This is the final multi-query attention output for `Fear`.

</Collapsible>

## grouped-query attention [#grouped-query-attention]

[Grouped-query attention](https://huggingface.co/papers/2305.13245) (GQA) is similar to MHA and MQA except each *group* of Qs share the same K and V. K and V are different for each group.

GQA combines the best of MHA and MQA. It's faster than MHA because it still has fewer Ks and Vs and it's more expressive than MQA because it has more Qs.

Most modern models like [Llama 3](https://huggingface.co/meta-llama/Meta-Llama-3-8B) and [gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) use GQA.

Try calculating the grouped-query attention score for the word `Fear` by hand for the following sequence. Head 0 and 1 make up Group 0 and Head 2 and 3 make up Group 1.

<Collapsible trigger="Show calculations">

<Table
  title="Group 0, Head 0"
  headers={["Word", "Q", "K", "V"]}
  data={[
    ["Fear", "2.0", "1.0", "2.0"],
    ["is", "3.0", "1.5", "3.0"],
  ]}
  className="my-8"
/>

<Table
  title="Group 0, Head 1"
  headers={["Word", "Q", "K", "V"]}
  data={[
    ["Fear", "4.0", "1.0", "2.0"],
    ["is", "6.0", "1.5", "3.0"],
  ]}
  className="my-8"
/>

<Table 
  title="Group 1, Head 2"
  headers={["Word", "Q", "K", "V"]}
  data={[
    ["Fear", "1.0", "2.0", "4.0"],
    ["is", "1.5", "3.0", "6.0"],
  ]}
  className="my-8"
/>

<Table 
  title="Group 1, Head 3"
  headers={["Word", "Q", "K", "V"]}
  data={[
    ["Fear", "3.0", "2.0", "4.0"],
    ["is", "4.5", "3.0", "6.0"],
  ]}
  className="my-8"
/>

1. For each head in group 0, multiply Q<sub>Fear</sub> by K for every word in the sequence to get the attention score.

    <div className="flex gap-8 items-start">
    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-green-500">Group 0</span></div>

    Head 0 -> `[2.0*1.0, 2.0*1.5]` = `[2.0, 3.0]`

    Head 1 -> `[4.0*1.0, 4.0*1.5]` = `[4.0, 6.0]`

    </div>

    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-orange-500">Group 1</span></div>

    Head 2 -> `[1.0*2.0, 1.0*3.0]` = `[2.0, 3.0]`

    Head 3 -> `[3.0*2.0, 3.0*3.0]` = `[9.0, 6.0]`

    </div>
    </div>

2. Scale the attention scores of each head by dividing by the square root of 2 (the vector dimension).

    <div className="flex gap-8 items-start">
    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-green-500">Group 0</span></div>

    Head 0 -> `[2.0/1.414, 3.0/1.414]` = `[1.414, 2.121]`

    Head 1 -> `[4.0/1.414, 6.0/1.414]` = `[2.828, 4.243]`

    </div>

    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-orange-500">Group 1</span></div>

    Head 2 -> `[2.0/1.414, 3.0/1.414]` = `[1.414, 2.121]`

    Head 3 -> `[9.0/1.414, 6.0/1.414]` = `[6.364, 4.243]`

    </div>
    </div>

3. Apply the softmax function to convert the attention scores into probabilities that add up to 1.

    <div className="flex gap-8 items-start">
    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-green-500">Group 0</span></div>

    Head 0 -> `[e^1.414 / (e^1.414 + e^2.121), e^2.121 / (e^1.414 + e^2.121)]` = `[0.33, 0.67]`

    Head 1 -> `[e^2.828 / (e^2.828 + e^4.243), e^4.243 / (e^2.828 + e^4.243)]` = `[0.195, 0.805]`

    </div>

    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-orange-500">Group 1</span></div>

    Head 2 -> `[e^1.414 / (e^1.414 + e^2.121), e^2.121 / (e^1.414 + e^2.121)]` = `[0.33, 0.67]`

    Head 3 -> `[e^6.364 / (e^6.364 + e^4.243), e^4.243 / (e^6.364 + e^4.243)]` = `[0.892, 0.107]`

    </div>
    </div>

4. Weight V by the attention scores.

    <div className="flex gap-8 items-start">
    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-green-500">Group 0</span></div>

    Head 0 -> `[0.33*2.0 + 0.67*3.0]` = `2.67`

    Head 1 -> `[0.195*2.0 + 0.805*3.0]` = `2.81`

    </div>

    <div className="flex-1">
    <div className="pb-1 mb-2"><span className="border-b-2 border-dotted border-orange-500">Group 1</span></div>

    Head 2 -> `[0.33*4.0 + 0.67*6.0]` = `5.34`

    Head 3 -> `[0.892*4.0 + 0.107*6.0]` = `4.21`

    </div>
    </div>

5. Concatenate the heads of each group and multiply by a weight matrix to combine the information.

    `[2.67, 2.81. 5.34, 4.21] * [[1.0, 0.0], [0.5, 0.5], [1.0, 2.0], [0.1, 1.0]]` = `[9.836, 16.295]`

This is the final grouped-query attention output for `Fear`.

</Collapsible>

## resources [#resources]

- [Attention Is All You Need](https://huggingface.co/papers/1706.03762)
- [Fast Transformer Decoding: One Write-Head is All You Need](https://huggingface.co/papers/1911.02150)
- [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://huggingface.co/papers/2305.13245)
- Youtube videos by [3Blue1Brown](https://www.youtube.com/watch?v=eMlx5fFNoYc) and [Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=2533s)