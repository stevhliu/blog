export const metadata = {
  title: 'Welcome to the Hugging-Verse',
  description: 'A quick guide to what Hugging Face is',
  openGraph: {
    title: 'Welcome to the Hugging-Verse',
    description: 'A quick guide to what Hugging Face is',
    images: [{ url: '/og/hf-universe' }]
  }
}

<FloatingTOC
  items={[
    { text: "tldr", href: "#tldr" },
    { text: "starting classes", href: "#starting-classes" },
    { text: "skill tree", href: "#skill-tree", level: 3 },
    { text: "the hub", href: "#the-hub" },
    { text: "storage", href: "#storage", level: 3 },
    { text: "spaces", href: "#spaces", level: 3 },
    { text: "inference providers and endpoints", href: "#inference-providers", level: 3 },
    { text: "jobs", href: "#jobs", level: 3},
    { text: "data studio", href: "#data-studio", level: 3},
    { text: "buffs", href: "#buffs" },
    { text: "courses", href: "#courses", level: 3 },
    { text: "papers", href: "#papers", level: 3},
    { text: "research", href: "#research", level: 3 },
    { text: "companions", href: "#companions"},
    { text: "reachy", href: "#reachy", level: 3},
    { text: "huggingchat", href: "#huggingchat", level: 3 },
  ]}
/>

<Figure>
  <Image src="/images/hf-universe.png" alt="The Hugging-Verse" width={672} height={378} className="rounded-xl" />
</Figure>

I often see people ask, *what is Hugging Face*? The answer is usually some variant of "Hugging Face is the GitHub of machine learning".

It's not a bad answer, but it hides a lot of depth. The Hugging Face ecosystem, or Hugging-Verse, is expansive and encompasses nearly every aspect of machine learning. For this reason, it can be overwhelming if you're just getting started.

This is my Hugging-Verse walkthrough, inspired by games like Baldur's Gate 3, Elden Ring, and Kingdom Come Deliverance II. These games are enormous, and you can easily spend 100+ hours on a single playthrough in each of them. Through side quests and lore, they add a ton of richness and worldbuilding that create immersive gameplay.

But I've also had to look certain things up in guides because I was overwhelmed. So if you're feeling lost, I hope this helps.

<div className="relative">
  <a id="tldr" className="absolute -top-[20px]" />
  <Collapsible trigger="tldr" defaultOpen>

  - Classes are the core libraries you build with.
  - Skill trees are optional specialized libraries.
  - The Hub is a hosted platform where you can get ML services like compute and storage.
  - Buffs are learning resources.
  - Companions are interactive products.

  </Collapsible>
</div>

## starting classes [#starting-classes]

There are many libraries in the Hugging-Verse, each dedicated to a specific topic like transformer models, diffusion models, pretraining/finetuning, robotics, evaluation, and more.

Choose a starter class, [Transformers](https://huggingface.co/docs/transformers) or [Diffusers](https://huggingface.co/docs/diffusers), depending on whether you're interested in large language models or image/video generation. These give you access to models and the APIs to train or run inference with them. It's important to level up these class skills first, like learning how to finetune a model with the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) API, because later on, you'll find that some of the more specialized libraries build on top of Transformers.

As an example, TRL trainers extend the Transformers Trainer. If you're already familiar with Trainer, then you'll get a +x% faster learning bonus.

### skill tree [#skill-tree]

As you level up, you can start exploring the skill tree and decide *whether* and *where* you want to spend your points on more specialized skills.

For example, if you're interested in a "training" build, put some points in [Accelerate](https://huggingface.co/docs/accelerate/index) or [nanotron](https://github.com/huggingface/nanotron). If you want to do reinforcement learning, invest a point in [TRL](https://huggingface.co/docs/trl/index). Or if you want to do an "optimization" build, check out [kernels](https://huggingface.co/docs/kernels/index) to build and load faster compute operations or [bitsandbytes](https://huggingface.co/docs/bitsandbytes/index) to quantize models to use less memory.

The reason you should consider *whether* you want to invest in these skills is that some features from these specialized libraries get added directly to Transformers over time. That's why it's such a powerful class. It can scale to the late-game, where you may need more specific abilities.

This isn't to say you shouldn't learn a specialized library, because not *all* abilities are integrated into Transformers. You may find that you need something in TRL that isn't available in Transformers.

## the hub [#the-hub]

The [Hub](https://huggingface.co) is where you go for shops and services to get things done. Most of these services are free to use, but a <ProBadge /> subscription unlocks access to higher limits and more features.

### storage [#storage]

One of the main services the Hub offers is storage for models and datasets you create. It pairs the Git workflow with [Xet's](https://huggingface.co/docs/hub/xet/index) storage system. Xet is faster and more efficient than Git LFS because it uses content-defined chunking (CDC) to deduplicate data. Only the parts of a file that changed are uploaded, unlike Git LFS, which uploads the entire file again.

The starter storage is pretty generous and comes with around 8TB of public storage and 100GB of private storage.

### spaces [#spaces]

[Spaces](https://huggingface.co/docs/hub/spaces) lets you turn your models into ML apps with frameworks like [Gradio](https://huggingface.co/docs/hub/spaces-sdks-gradio), [Docker](https://huggingface.co/docs/hub/spaces-sdks-docker), and [React](https://huggingface.co/docs/hub/spaces-sdks-static). You can even turn your ML app into a [callable tool](https://huggingface.co/docs/hub/spaces-mcp-servers) for agents with MCP to integrate a Space into a workflow.

[Arena](https://arena.ai/blog/lmarena-is-now-arena/) (formerly LMArena) is a $1.7B startup that started off as a Gradio app comparing how models perform on different tasks. If you can imagine it, you can build it with Spaces.

A free Space runs on a CPU with 16GB of RAM, and you can upgrade to bigger GPUs if you need additional compute. For a more unique and powerful hardware option, try <ZeroBadge />, a shared cluster of H200s. The H200s are dynamically allocated to a Space to complete a workload, then released for the next Space. This ensures you only use compute when you need it and aren't leaving GPUs idle.

### inference providers and endpoints [#inference-providers]

Serverless inference lets you run your model through an API without having to manage the infrastructure on your own. This is designed to help you deploy models to production. There are two options:

- [Inference Providers](https://huggingface.co/docs/inference-providers/index) connect models on the Hub to companies like [Cerebras](https://www.cerebras.ai/) and [Hyperbolic](https://www.hyperbolic.ai/) to let you make on-demand inference calls to their hardware. Make sure you use the [comparison tool](https://huggingface.co/inference/models) to help you select a provider based on price or speed.

- [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) is Hugging Face's dedicated and managed inference infrastructure ("endpoint") that runs continuously. There are more choices to make about the deployment, such as hardware (AWS/GCP/Azure), inference engine (vLLM/SGLang), and autoscaling.

### jobs [#jobs]

[Jobs](https://huggingface.co/docs/hub/jobs-overview) provides access to Hugging Face's hardware (CPUs/GPUs) for *temporary* compute. It stops once the task is complete, or you can [schedule](https://huggingface.co/docs/hub/jobs-schedule) a task to run periodically.

<Snippet>
{`hf jobs uv run --flavor a100-large --timeout 6h --with trl --secrets HF_TOKEN train.py`}
</Snippet>
<Caption>Launch a finetuning job on an A100 GPU.</Caption>

This is useful for one-off or scheduled workloads like finetuning and data processing.

### data studio [#data-studio]

[Data Studio](https://huggingface.co/docs/hub/data-studio) is available in dataset repositories for exploring data in the browser without downloading it. Ask the agent questions about a dataset or use the built-in SQL console to query it.

<iframe
  src="https://huggingface.co/datasets/HuggingFaceFW/finewiki/embed/viewer/en/train"
  title="FineWiki dataset viewer"
  width="100%"
  height="560px"
  loading="lazy"
></iframe>

## buffs [#buffs]

Acquire these permanent buffs to increase your research and knowledge skills.

### courses [#courses]

Take the courses at [hf.co/learn](https://hf.co/learn), which cover topics like agents, reinforcement learning, diffusion, and more.

This is a good early-game buff because you get more in-depth explanations about how things work.

### papers [#papers]

Browse [Papers](https://huggingface.co/papers/trending), a curated daily selection to help you stay on top of the latest research.

### research [#research]

Follow the [science team](https://huggingface.co/science), which produces and shares research you can learn from and build on.

- [FineData](https://huggingface.co/HuggingFaceFW) has several clean and high-quality datasets for large-scale pretraining, and they've also shared their recipe for extracting and refining data.
- [Smol Models Research](https://huggingface.co/HuggingFaceTB) releases small but competitive models, and has also written a playbook for training them.

This is a good late-game buff.

<iframe
	src="https://huggingfacetb-smol-training-playbook.hf.space"
	title="Smol Training Playbook"
	className="mx-auto block"
	width="600"
	height="450"
	loading="lazy"
></iframe>

## companions [#companions]

Add a companion to your party to help you out.

### reachy [#reachy]

[Reachy](https://huggingface.co/spaces/pollen-robotics/Reachy_Mini) is a desktop robot for experimenting with human-robot interactions. The robot is built on open-source software, so you can program new "behaviors" for it.

<Tweet
    id="2008319790868201795"
/>

### huggingchat [#huggingchat]

[HuggingChat](https://huggingface.co/chat/) is an open version of ChatGPT with support for many models like Kimi-K2.5 and gpt-oss-120B. If you don't know which model to use, its Omni router automatically selects the best model for your message.

HuggingChat is also available in the Hugging Face docs like Transformers, as well as in Papers, so you can ask it questions directly to level up even faster.