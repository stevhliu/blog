export const metadata = {
  title: 'Chat templates',
  description: 'A guide to chat templates',
  openGraph: {
    title: 'Chat templates',
    description: 'A guide to chat templates',
    images: [{ url: '/og/chat-templates' }]
  }
};

<FloatingTOC
  items={[
    { text: "transformers", href: "#transformers" },
    { text: "single-input", href: "#single-input" },
    { text: "multi-inputs", href: "#multi-inputs" },
    { text: "mixed inputs", href: "#mixed-inputs" },
    { text: "batched inputs", href: "#batched-inputs" },
    { text: "summary", href: "#summary" },
    { text: "resources", href: "#resources" }
  ]}
/>

When you send a message to ChatGPT, it looks something like this, from the UI interface to the underlying API.

<Carousel>
  <Figure>
    <img className="rounded-xl" src="https://huggingface.co/datasets/stevhliu/personal-blog/resolve/main/chatgpt-input.png" alt="Chat input to ChatGPT" />
  </Figure>

  <Snippet language="json">
    {`
    [
      { "role": "developer", "content": "You are a professional pitmaster." },
      { "role": "user", "content": "Give me a step-by-step recipe for smoking a 12 lb brisket." }
    ]
    `}
  </Snippet>
</Carousel>

It's easy for us to read, but language models don't see things this way.

A language model predicts the next token in a sequence based on the previous tokens. Data formats like JSON are interpreted as a string of tokens. There is no structure or boundaries that let the model know who is speaking and when to respond.

<Snippet>
{`
[ { " r o l e " : " u s e r " , " c o n t e n t " : "..." ]
`}
</Snippet>

A *chat template* converts structured chat data into a single tokenizable string a model can process.
  
<Snippet language="yaml">
{`<|start|>developer<|message|># Instructions
  You are a professional pitmaster.
  <|end|>
  <|start|>user<|message|>
  Give me a step-by-step recipe for smoking a 12 lb brisket.
  <|end|>
  <|start|>assistant
`}
</Snippet>
<Caption>
[gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) chat template
</Caption>

Chat templates provide structure and boundaries with special tokens.

- `<|start|>` and `<|end|>` tokens indicate the start and end of a message
- conversational roles like `developer`, `user` and `assistant` are labeled
- `<|message|>` token indicates the message content

The template ensures a model knows who's speaking and maintains conversation order. Without structure, a model may lose track of speaker roles or produce low-quality and inconsistent responses.

Models may be trained with different chat templates that use different tokens. For example, [Llama 4](https://www.llama.com/docs/model-cards-and-prompt-formats/llama4/#-llama-4---prompt-formats-) uses the `<|begin_of_text|>` token instead of `<|start|>`. This is important because if you use a chat template that doesn't match the one a model was trained with, you may end up with poor responses. A model trained with `<|start|>` may not understand it is the beginning of a message if you use `<|begin_of_text|>` instead.

So how do you ensure you're always using the correct chat template?

## Transformers

Many chat models on the [Hub](https://huggingface.co/models) are released with a [chat_template.jinja](https://huggingface.co/openai/gpt-oss-20b/blob/main/chat_template.jinja) file. You can take a look at a model's template by clicking on the "Chat template" button on the model page.

<Figure>
  <img className="rounded-xl" src="https://huggingface.co/datasets/stevhliu/personal-blog/resolve/main/hub-chat-template.png" alt="Hub chat template" />
</Figure>

In Transformers, the [apply_chat_template](https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template) function loads the template file and fills it with your chat data. Use the `chat_template` attribute on a tokenizer or processor (for multimodal chat models) to view the template.

<Snippet language="python">
{`
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("openai/gpt-oss-20b")
print(tokenizer.chat_template)
`}
</Snippet>



## single-input [#single-input]

## multi-inputs [#multi-inputs]

## mixed inputs [#mixed-inputs]

## batched inputs [#batched-inputs]

## summary [#summary]

## resources [#resources]

- The Transformers [Chat templates](https://huggingface.co/docs/transformers/chat_templating) guide and the chat docs in that section.
